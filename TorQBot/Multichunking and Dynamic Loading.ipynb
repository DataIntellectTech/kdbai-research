{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8833c09a-c76b-4839-8b41-37f0b34fd3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kill the add_texts Function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d46d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector DB\n",
    "import os\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "import kdbai_client as kdbai\n",
    "import time\n",
    "import multiprocessing\n",
    "# langchain packages\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import KDBAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "os.environ['OPENAI_API_KEY'] = \"<your API key>\"\n",
    "# Load the documents we want to prompt an LLM about\n",
    "loader = TextLoader('./data/TorQ+Conf.txt')\n",
    "doc = loader.load()\n",
    "# Chunk the documents into chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "text_splitter1 = RecursiveCharacterTextSplitter(chunk_size=30, chunk_overlap=10)\n",
    "text_splitter2 = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=30)\n",
    "text_splitter3 = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "text_splitter4 = RecursiveCharacterTextSplitter(chunk_size=75, chunk_overlap=50)\n",
    "text_splitter5 = RecursiveCharacterTextSplitter(chunk_size=10, chunk_overlap=5)\n",
    "text_splitter6 = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "text_splitter7 = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=90)\n",
    "text_splitter8 = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "text_splitter9 = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "text_splitter10 = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=50)\n",
    "text_splitter11 = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=100)\n",
    "text_splitter12 = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "text_splitter13 = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=40)\n",
    "text_splitter14 = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=35)\n",
    "text_splitter15 = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "text_splitter16 = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "text_splitter17 = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=100)\n",
    "text_splitter18 = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=150)\n",
    "#split_documents produces a list of all the chunks created, printing out first chunk for example\n",
    "chunks1 = text_splitter1.split_documents(doc)\n",
    "chunks2 = text_splitter2.split_documents(doc)\n",
    "chunks3 = text_splitter3.split_documents(doc)\n",
    "chunks4 = text_splitter4.split_documents(doc)\n",
    "chunks5 = text_splitter5.split_documents(doc)\n",
    "chunks6 = text_splitter6.split_documents(doc)\n",
    "chunks7 = text_splitter7.split_documents(doc)\n",
    "chunks8 = text_splitter8.split_documents(doc)\n",
    "chunks9 = text_splitter9.split_documents(doc)\n",
    "chunks10 = text_splitter10.split_documents(doc)\n",
    "chunks11 = text_splitter11.split_documents(doc)\n",
    "chunks12 = text_splitter12.split_documents(doc)\n",
    "chunks13 = text_splitter13.split_documents(doc)\n",
    "chunks14 = text_splitter14.split_documents(doc)\n",
    "chunks15 = text_splitter15.split_documents(doc)\n",
    "chunks16 = text_splitter16.split_documents(doc)\n",
    "chunks17 = text_splitter17.split_documents(doc)\n",
    "chunks18 = text_splitter18.split_documents(doc)\n",
    "texts1 = [p.page_content for p in chunks1]\n",
    "texts2 = [p.page_content for p in chunks2]\n",
    "texts3 = [p.page_content for p in chunks3]\n",
    "texts4 = [p.page_content for p in chunks4]\n",
    "texts5 = [p.page_content for p in chunks5]\n",
    "texts6 = [p.page_content for p in chunks6]\n",
    "texts7 = [p.page_content for p in chunks7]\n",
    "texts8 = [p.page_content for p in chunks8]\n",
    "texts9 = [p.page_content for p in chunks9]\n",
    "texts10 = [p.page_content for p in chunks10]\n",
    "texts11 = [p.page_content for p in chunks11]\n",
    "texts12 = [p.page_content for p in chunks12]\n",
    "texts13 = [p.page_content for p in chunks13]\n",
    "texts14 = [p.page_content for p in chunks14]\n",
    "texts15 = [p.page_content for p in chunks15]\n",
    "texts16 = [p.page_content for p in chunks16]\n",
    "texts17 = [p.page_content for p in chunks17]\n",
    "texts18 = [p.page_content for p in chunks18]\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "session = kdbai.Session(endpoint=\"http://localhost:8082\")\n",
    "# Define Schema\n",
    "rag_schema = {\n",
    "    \"columns\": [\n",
    "        {\"name\": \"id\", \"pytype\": \"str\"},\n",
    "        {\"name\": \"text\", \"pytype\": \"bytes\"},\n",
    "        {\n",
    "            \"name\": \"embeddings\",\n",
    "            \"pytype\": \"float32\",\n",
    "            \"vectorIndex\": {\"dims\": 1536, \"metric\": \"L2\", \"type\": \"flat\"},\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "# First ensure the table does not already exist\n",
    "try:\n",
    "    session.table(\"rag_langchain\").drop()\n",
    "    time.sleep(5)\n",
    "except kdbai.KDBAIException:\n",
    "    pass\n",
    "table = session.create_table(\"rag_langchain\", rag_schema)\n",
    "# use KDBAI as vector store\n",
    "vecdb_kdbai = KDBAI(table, embeddings)\n",
    "# Foo function\n",
    "def foo(n):\n",
    "    vecdb_kdbai.add_texts(texts=texts1)\n",
    "if __name__ == '__main__':\n",
    "    # Start foo as a process\n",
    "    p = multiprocessing.Process(target=foo, name=\"Foo\", args=(5,))\n",
    "    p.start()\n",
    "# Wait for a 5 seconds max for foo\n",
    "# Usage: join([timeout in seconds])\n",
    "p.join(5)\n",
    "\n",
    "# If thread is active\n",
    "if p.is_alive():\n",
    "    print(\"Kill the add_texts Function\")\n",
    "\n",
    "    # Terminate foo\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "\n",
    "K = 10\n",
    "qabot = RetrievalQA.from_chain_type(chain_type='stuff',\n",
    "                                    llm=ChatOpenAI(model='gpt-3.5-turbo-16k', temperature=0.0), \n",
    "                                    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K)),\n",
    "                                    return_source_documents=True)\n",
    "vecdb_kdbai.aadd_texts(texts=texts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d283740f-64f8-4bc4-9908-f43e841c064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kill the add_texts Function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbcc7907610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foo function\n",
    "def foo1(n):\n",
    "    vecdb_kdbai.add_texts(texts=texts2)\n",
    "if __name__ == '__main__':\n",
    "    # Start foo as a process\n",
    "    p = multiprocessing.Process(target=foo1, name=\"Foo\", args=(5,))\n",
    "    p.start()\n",
    "# Wait for a 5 seconds max for foo\n",
    "# Usage: join([timeout in seconds])\n",
    "p.join(5)\n",
    "\n",
    "# If thread is active\n",
    "if p.is_alive():\n",
    "    print(\"Kill the add_texts Function\")\n",
    "\n",
    "    # Terminate foo\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "vecdb_kdbai.aadd_texts(texts=texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df88b4c9-240c-4641-8ea3-91cbad56c986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kill the add_texts Function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d5310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foo function\n",
    "def foo2(n):\n",
    "    vecdb_kdbai.add_texts(texts=texts3)\n",
    "if __name__ == '__main__':\n",
    "    # Start foo as a process\n",
    "    p = multiprocessing.Process(target=foo2, name=\"Foo\", args=(5,))\n",
    "    p.start()\n",
    "# Wait for a 5 seconds max for foo\n",
    "# Usage: join([timeout in seconds])\n",
    "p.join(5)\n",
    "\n",
    "# If thread is active\n",
    "if p.is_alive():\n",
    "    print(\"Kill the add_texts Function\")\n",
    "\n",
    "    # Terminate foo\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "vecdb_kdbai.aadd_texts(texts=texts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2e92f5-4e7d-4100-aee6-6493c692a86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kill the add_texts Function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d5380>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foo function\n",
    "def foo3(n):\n",
    "    vecdb_kdbai.add_texts(texts=texts1)\n",
    "if __name__ == '__main__':\n",
    "    # Start foo as a process\n",
    "    p = multiprocessing.Process(target=foo3, name=\"Foo\", args=(5,))\n",
    "    p.start()\n",
    "# Wait for a 5 seconds max for foo\n",
    "# Usage: join([timeout in seconds])\n",
    "p.join(5)\n",
    "\n",
    "# If thread is active\n",
    "if p.is_alive():\n",
    "    print(\"Kill the add_texts Function\")\n",
    "\n",
    "    # Terminate foo\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "vecdb_kdbai.aadd_texts(texts=texts4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e908769-64da-43e9-8e46-84f4c02c6b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kill the add_texts Function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d51c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foo function\n",
    "def foo4(n):\n",
    "    vecdb_kdbai.add_texts(texts=texts5)\n",
    "if __name__ == '__main__':\n",
    "    # Start foo as a process\n",
    "    p = multiprocessing.Process(target=foo4, name=\"Foo\", args=(5,))\n",
    "    p.start()\n",
    "# Wait for a 5 seconds max for foo\n",
    "# Usage: join([timeout in seconds])\n",
    "p.join(5)\n",
    "\n",
    "# If thread is active\n",
    "if p.is_alive():\n",
    "    print(\"Kill the add_texts Function\")\n",
    "\n",
    "    # Terminate foo\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "vecdb_kdbai.aadd_texts(texts=texts5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ee46e6-e3e6-4150-8c0d-db9ec631a711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kill the add_texts Function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d4f90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foo function\n",
    "def foo5(n):\n",
    "    vecdb_kdbai.add_texts(texts=texts6)\n",
    "if __name__ == '__main__':\n",
    "    # Start foo as a process\n",
    "    p = multiprocessing.Process(target=foo5, name=\"Foo\", args=(5,))\n",
    "    p.start()\n",
    "# Wait for a 5 seconds max for foo\n",
    "# Usage: join([timeout in seconds])\n",
    "p.join(5)\n",
    "\n",
    "# If thread is active\n",
    "if p.is_alive():\n",
    "    print(\"Kill the add_texts Function\")\n",
    "\n",
    "    # Terminate foo\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "vecdb_kdbai.aadd_texts(texts=texts6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15bab6fe-48a3-4604-b539-69add7cd94aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kill the add_texts Function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d4e40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foo function\n",
    "def foo6(n):\n",
    "    vecdb_kdbai.add_texts(texts=texts7)\n",
    "if __name__ == '__main__':\n",
    "    # Start foo as a process\n",
    "    p = multiprocessing.Process(target=foo6, name=\"Foo\", args=(5,))\n",
    "    p.start()\n",
    "# Wait for a 5 seconds max for foo\n",
    "# Usage: join([timeout in seconds])\n",
    "p.join(5)\n",
    "\n",
    "# If thread is active\n",
    "if p.is_alive():\n",
    "    print(\"Kill the add_texts Function\")\n",
    "\n",
    "    # Terminate foo\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "vecdb_kdbai.aadd_texts(texts=texts7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f08307ac-5a7d-4d93-bb3d-4e0ad6266772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d57e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecdb_kdbai.aadd_texts(texts=texts8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe1e103-4391-46a7-b9ee-4aff79b1c1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d5620>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecdb_kdbai.aadd_texts(texts=texts9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e672ab81-112e-4f74-9ca0-b011f0192b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d4c10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecdb_kdbai.aadd_texts(texts=texts10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66411c8a-84e9-427d-9262-eaeaf2491c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d5690>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecdb_kdbai.aadd_texts(texts=texts11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63dca7c7-c6c4-4004-9475-85a4a25c9e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d5e00>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecdb_kdbai.aadd_texts(texts=texts12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc5eadba-9afc-4378-992f-e49c5d53bd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d5770>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecdb_kdbai.aadd_texts(texts=texts13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e33573c6-eb43-402e-87e1-51ed04702405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d5e70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecdb_kdbai.aadd_texts(texts=texts14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b9be9e7-db7f-4039-a637-34a0f39935f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d5d20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecdb_kdbai.aadd_texts(texts=texts15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63985483-2a29-456d-8585-c410ff826dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d5d90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecdb_kdbai.aadd_texts(texts=texts16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e331817b-606c-492b-9acf-4851cc2d2f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d5150>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecdb_kdbai.aadd_texts(texts=texts17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3152d140-a821-4d13-b19a-8d5a3d25f762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d5a80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecdb_kdbai.aadd_texts(texts=texts18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5128813f-5df7-4a9f-8aa1-6662117d6ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kill the add_texts Function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d6a40>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfocus = TextLoader('./scrapings/Confluence.txt')\n",
    "docfocus = loadfocus.load()\n",
    "text_splitterfocus = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=50)\n",
    "chunksfocus = text_splitterfocus.split_documents(docfocus)\n",
    "textsfocus = [p.page_content for p in chunksfocus]\n",
    "# Foo function\n",
    "def foofocus(n):\n",
    "    vecdb_kdbai.add_texts(texts=textsfocus)\n",
    "if __name__ == '__main__':\n",
    "    # Start foo as a process\n",
    "    p = multiprocessing.Process(target=foofocus, name=\"Foo\", args=(5,))\n",
    "    p.start()\n",
    "# Wait for a 5 seconds max for foo\n",
    "# Usage: join([timeout in seconds])\n",
    "p.join(5)\n",
    "\n",
    "# If thread is active\n",
    "if p.is_alive():\n",
    "    print(\"Kill the add_texts Function\")\n",
    "\n",
    "    # Terminate foo\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "vecdb_kdbai.aadd_texts(texts=textsfocus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b7e9b33-af34-4c7b-8211-5968b0658d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7fbc765d5f50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfocus2 = TextLoader('./scrapings/personalsettings.txt')\n",
    "docfocus2 = loadfocus2.load()\n",
    "text_splitterfocus2 = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=30)\n",
    "chunksfocus2 = text_splitterfocus2.split_documents(docfocus2)\n",
    "textsfocus2 = [p.page_content for p in chunksfocus2]\n",
    "# Foo function\n",
    "def foofocus2(n):\n",
    "    vecdb_kdbai.add_texts(texts=textsfocus2)\n",
    "if __name__ == '__main__':\n",
    "    # Start foo as a process\n",
    "    p = multiprocessing.Process(target=foofocus2, name=\"Foo\", args=(5,))\n",
    "    p.start()\n",
    "# Wait for a 5 seconds max for foo\n",
    "# Usage: join([timeout in seconds])\n",
    "p.join(5)\n",
    "\n",
    "# If thread is active\n",
    "if p.is_alive():\n",
    "    print(\"Kill the add_texts Function\")\n",
    "\n",
    "    # Terminate foo\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "vecdb_kdbai.aadd_texts(texts=textsfocus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44f3b1c6-4fcb-4da1-8c2a-f619672f043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What code do I use to query my quote table for Google and IBM quotes between last week and today in TorQ? Please make substitutions using my personal settings\n",
      "\n",
      "To query your quote table for Google and IBM quotes between last week and today in TorQ, you can use the following code:\n",
      "\n",
      "```\n",
      "select from QuoteBoat where sym in `GOOG`IBM, date within (.z.d-7; .z.d)\n",
      "```\n",
      "\n",
      "Please note that you need to replace `QuoteBoat` with the actual name of your quotes table in TorQ. Additionally, make sure that the tick symbols for Google and IBM (`GOOG` and `IBM`) are correct for your dataset.\n",
      "\n",
      "Let me know if you need any further assistance!\n"
     ]
    }
   ],
   "source": [
    "query = \"What code do I use to query my quote table for Google and IBM quotes between last week and today in TorQ? Please make substitutions using my personal settings\"\n",
    "print(f'{query}\\n')\n",
    "print(qabot.invoke(dict(query=query))['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea0d04-8aab-49ec-8691-2c028df89436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
