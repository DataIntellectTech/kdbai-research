{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f024ecc8-5d6b-4542-85b0-5dab115c515c",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "Load the various libraries that will be needed in this tutorial, including all the langchain libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d0416-6255-470a-9099-4d22020ec81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector DB\n",
    "import os\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "import kdbai_client as kdbai\n",
    "import time\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc2e506-bf60-4fad-a7a8-5e78b7159fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain packages\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import KDBAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec76ba-de06-47a9-8a87-53f8f8e5d790",
   "metadata": {},
   "source": [
    "## Set API Keys\n",
    "\n",
    "To follow this example you will need to request an [Open API key](https://platform.openai.com/apps).\n",
    "\n",
    "You can create one for free using the link provided. Once you have the credentials you can add them below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d782662b-0e8e-4d38-ab82-ffd01d3211ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    os.environ[\"OPENAI_API_KEY\"]\n",
    "    if \"OPENAI_API_KEY\" in os.environ\n",
    "    else getpass(\"OpenAI API Key:\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31fb78f-c114-4c66-bcf4-7c83e4bd87bf",
   "metadata": {},
   "source": [
    "## Load and split the data into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915cd93-5916-4719-bc6c-8e7e806d025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN THE TorQ DOCUMENTATION\n",
    "loader = TextLoader('./TorQ+Conf.txt')\n",
    "doc = loader.load()\n",
    "# Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "\n",
    "#split_documents produces a list of all the chunks created, printing out first chunk for example\n",
    "chunks = [p.page_content for p in text_splitter.split_documents(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefb7dd-4397-4511-8996-5bac6bee4544",
   "metadata": {},
   "source": [
    "## Store Embeddings in KDB.AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef854f4-bc77-4c38-ace3-c502731c984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an OpenAI text embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dba921-a7ab-4519-b870-ad011bf6af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish KDB.AI Server session\n",
    "session = kdbai.Session(endpoint=\"http://localhost:8082\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f57daa-5250-4ebf-a00e-f64b0fe6843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Schema\n",
    "rag_schema = {\n",
    "    \"columns\": [\n",
    "        {\"name\": \"id\", \"pytype\": \"str\"},\n",
    "        {\"name\": \"text\", \"pytype\": \"bytes\"},\n",
    "        {\n",
    "            \"name\": \"embeddings\",\n",
    "            \"pytype\": \"float32\",\n",
    "            \"vectorIndex\": {\"dims\": 1536, \"metric\": \"L2\", \"type\": \"flat\"},\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fbbe0a-f63f-4923-af10-eff988ed046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ensure the table does not already exist\n",
    "try:\n",
    "    session.table(\"rag_langchain\").drop()\n",
    "    time.sleep(5)\n",
    "except kdbai.KDBAIException:\n",
    "    pass\n",
    "table = session.create_table(\"rag_langchain\", rag_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84324cf1-34b4-42c4-b86f-24ef48d633bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kill the add_texts Function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coroutine object VectorStore.aadd_texts at 0x7efdc09920a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use KDBAI as vector store\n",
    "vecdb_kdbai = KDBAI(table, embeddings)\n",
    "# Foo function\n",
    "def foo(n):\n",
    "    vecdb_kdbai.add_texts(texts=chunks)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Start foo as a process\n",
    "    p = multiprocessing.Process(target=foo, name=\"Foo\", args=(5,))\n",
    "    p.start()\n",
    "# Wait for a 5 seconds max for foo\n",
    "# Usage: join([timeout in seconds])\n",
    "p.join(5)\n",
    "\n",
    "# If thread is active\n",
    "if p.is_alive():\n",
    "    print(\"Kill the add_texts Function\")\n",
    "\n",
    "    # Terminate foo\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "vecdb_kdbai.aadd_texts(texts=chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea538e6-318e-4db8-93c6-45b8df3ab85b",
   "metadata": {},
   "source": [
    "## Create Q&A RAG bot\n",
    "\n",
    "Create a Q&A bot using the OpenAI model gpt-3.5-turbo-16k and the vecdb_kdbai vector store containing the embedded TorQ documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8062854-9960-48ad-8d3c-038b68b37fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "qabot = RetrievalQA.from_chain_type(chain_type='stuff',\n",
    "                                    llm=ChatOpenAI(model='gpt-3.5-turbo-16k', temperature=0.0), \n",
    "                                    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K)),\n",
    "                                    return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fadb0b6c-189f-4094-a30e-385a82f836fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "What is STP?\n",
      "\n",
      "Based on the given context, there is no mention of STP (Shortest Path) or any other relevant acronym that could be associated with \"STP.\" Therefore, I don't have enough information to answer your question.\n"
     ]
    }
   ],
   "source": [
    "query =\"What is STP?\"\n",
    "print(f'\\n\\n{query}\\n')\n",
    "print(qabot.invoke(dict(query=query))['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5928ae51-579a-4b82-befb-e66751535eb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.table(\"rag_langchain\").drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d64b61c-cc16-4797-b950-5c4fb0748b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
