{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f97d9a-569b-4174-be82-5df869b3f39f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e8169-e4b1-49a4-974a-87ef7c8277d7",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84324cf1-34b4-42c4-b86f-24ef48d633bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "import kdbai_client as kdbai\n",
    "import time\n",
    "import multiprocessing\n",
    "# langchain packages\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import KDBAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad2878-abc8-48b8-9d40-a917ef38b14a",
   "metadata": {},
   "source": [
    "### Start KDB.AI session, define table schema and chunk the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "281fd99c-e74d-4134-b485-d3ad923159fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add OpenAI credentials\n",
    "os.environ['OPENAI_API_KEY'] = \"<your API key>\"\n",
    "\n",
    "#Load the documents we want to prompt an LLM about\n",
    "loader = TextLoader('./data/TorQ+Conf.txt')\n",
    "doc = loader.load()\n",
    "# Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "#split_documents produces a list of all the chunks created, printing out first chunk for example\n",
    "chunks = text_splitter.split_documents(doc) \n",
    "pages = [p.page_content for p in chunks]\n",
    "\n",
    "# Define OpenAI Text Embedding Model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Establish a kdb.ai instance\n",
    "session = kdbai.Session(endpoint=\"http://localhost:8082\")\n",
    "\n",
    "# Define a vector DB table schema for storing embeddings\n",
    "rag_schema = {\n",
    "    \"columns\": [\n",
    "        {\"name\": \"id\", \"pytype\": \"str\"},\n",
    "        {\"name\": \"text\", \"pytype\": \"bytes\"},\n",
    "        {\n",
    "            \"name\": \"embeddings\",\n",
    "            \"pytype\": \"float32\",\n",
    "            \"vectorIndex\": {\"dims\": 1536, \"metric\": \"L2\", \"type\": \"flat\"},\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beceb0c-6c3b-4ac8-9e2a-6ac7cdad18f2",
   "metadata": {},
   "source": [
    "### Store data in vector database. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6571050e-248b-474d-9853-06231eb7b4dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "To embed and add our texts we must first run 'vecdb_kdbai.add_texts(texts=texts)' for a few seconds, before interupting the kernel and running 'vecdb_kdbai.aadd_texts(texts=texts)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400079d1-6ef0-4167-ad97-93c96fe3a2b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = session.create_table(\"rag_langchain\", rag_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e975e59-c8e8-46ee-8f49-a959c71ec57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ensure the table does not already exist\n",
    "try:\n",
    "    session.table(\"rag_langchain\").drop()\n",
    "    time.sleep(5)\n",
    "except kdbai.KDBAIException:\n",
    "    pass\n",
    "table = session.create_table(\"rag_langchain\", rag_schema)\n",
    "# use KDBAI as vector store\n",
    "vecdb_kdbai = KDBAI(table, embeddings)\n",
    "# Foo function\n",
    "def foo(n):\n",
    "    vecdb_kdbai.add_texts(texts=pages)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Start foo as a process\n",
    "    p = multiprocessing.Process(target=foo, name=\"Foo\", args=(5,))\n",
    "    p.start()\n",
    "# Wait for a 5 seconds max for foo\n",
    "# Usage: join([timeout in seconds])\n",
    "p.join(5)\n",
    "\n",
    "# If thread is active\n",
    "if p.is_alive():\n",
    "    print(\"Kill the add_texts Function\")\n",
    "\n",
    "    # Terminate foo\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "vecdb_kdbai.aadd_texts(texts=pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1292d-7b9a-4123-b9b6-1aa5b6d1d37d",
   "metadata": {},
   "source": [
    "### Create chat store 'messages' to record the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee647467-0c1b-40ab-9d7f-30ae17e180ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a TorQ Expert Smart Support bot called 'TESS'.\"),\n",
    "    SystemMessage(content=\"Interpret the following acronyms when answering questions. RDB is an acronym for real time database, WDB is an acronym for write database, CTP is an acronym for chained tickerplant, STP is an acronym for segmented tickerplant, TP is an acronym for tickerplant, and HDB is an acronym for historical database.\"),\n",
    "]\n",
    "\n",
    "K=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4b14f2-2ad1-4aa3-8518-bd13fbe0847d",
   "metadata": {},
   "source": [
    "### Define function to contextualise prompts with the previous conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec97177-9b7b-4d7e-ab49-55864381df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_history():\n",
    "    global messages\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a TorQ Expert Smart Support bot called 'TESS'.\"),\n",
    "        SystemMessage(content=\"Interpret the following acronyms when answering questions. RDB is an acronym for real time database, WDB is an acronym for write database, CTP is an acronym for chained tickerplant, STP is an acronym for segmented tickerplant, TP is an acronym for tickerplant, and HDB is an acronym for historical database.\"),\n",
    "    ]\n",
    "    \n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo-16k', temperature=0.0)\n",
    "\n",
    "def rel_info(msgs):\n",
    "    if len(msgs) > 3:\n",
    "        return msgs[:2], msgs[-1]\n",
    "    else:\n",
    "        return msgs\n",
    "\n",
    "def contextualise_query(query: str):\n",
    "    contextualize_q_system_prompt = f\"\"\"If it seems like a user asks about something related to the Chat History \n",
    "    Given the following conversation and a follow up question, \n",
    "    REPHRASE ONLY the follow up question to be a clear concise question that can be understood without context\n",
    "    based on the conversation. \n",
    "    Rewrite 'RDB' with 'real time database', \n",
    "    Rewrite 'HDB' with 'historical database', \n",
    "    Rewrite 'WDB' with 'write database', \n",
    "    Rewrite 'STP' with 'segmented tickerplant', \n",
    "    Rewrite 'CTP' with 'chained tickerplant'.\n",
    "    If \"in TorQ\" is not mentioned in the question, mention it at the end. \n",
    "    After the question write 'Explain fully.'.\n",
    "    \n",
    "    Chat History:\n",
    "    {rel_info(messages)}\n",
    "\n",
    "    Follow up question:\n",
    "    {query}\"\"\"\n",
    "    return chat.invoke(contextualize_q_system_prompt)\n",
    "    \n",
    "# After the question write 'Explain fully.'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c777eb33-fa02-4e9a-a59d-c32fef3c0bca",
   "metadata": {},
   "source": [
    "### Define TESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8062854-9960-48ad-8d3c-038b68b37fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "qabot = RetrievalQA.from_chain_type(chain_type='stuff',\n",
    "                                    llm=ChatOpenAI(model='gpt-3.5-turbo-16k', temperature=0.0), \n",
    "                                    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K)),\n",
    "                                    return_source_documents=True)\n",
    "def TESS(query):\n",
    "    query = contextualise_query(query).content\n",
    "    messages.extend([HumanMessage(content=query)])\n",
    "    print(f'\\n\\n{query}\\n')\n",
    "    messages.extend([AIMessage(content=qabot.invoke(dict(query=query))['result'])])\n",
    "    print(messages[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e1a0b1-a1ec-4850-be2e-26835f13aa1a",
   "metadata": {},
   "source": [
    "# Time to run some queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fadb0b6c-189f-4094-a30e-385a82f836fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "How do I restart a process in TorQ? Explain fully.\n",
      "\n",
      "To restart a process in TorQ, you need to follow these steps:\n",
      "\n",
      "1. Identify the process you want to restart. This can be done by checking the list of available processes in the `.servers.SERVERS` table. Each process has a unique `proctype` and `procname` associated with it.\n",
      "\n",
      "2. Open a command prompt or terminal and navigate to the TorQ installation directory.\n",
      "\n",
      "3. Run the following command to restart the process:\n",
      "   ```\n",
      "   q torq.q -load code/processes/<process_name>.q -p <port_number> -proctype <process_type> -procname <process_name>\n",
      "   ```\n",
      "   Replace `<process_name>` with the name of the process you want to restart, `<port_number>` with the port number the process is running on, `<process_type>` with the type of the process, and `<process_name>` with the name of the process.\n",
      "\n",
      "   For example, if you want to restart the `reporter1` process running on port `20004`, the command would be:\n",
      "   ```\n",
      "   q torq.q -load code/processes/reporter.q -p 20004 -proctype reporter -procname reporter1\n",
      "   ```\n",
      "\n",
      "4. Press Enter to execute the command. The process will be restarted and any changes made to the process code will take effect.\n",
      "\n",
      "It's important to note that TorQ allows for extensibility, so you can modify or extend the processes as required. Additionally, TorQ provides inbuilt result handlers for writing reports to file or publishing them. The process restart command logs each step of the report process and is fully integrated with the TorQ gateway, allowing reports to be run across processes.\n",
      "\n",
      "For more detailed information on TorQ processes and their implementation, you can refer to the TorQ Manual.\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I restart a process?\"\n",
    "TESS(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89507860-c2f4-4c25-8fb3-971497e922ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c663871-8f4e-49de-ae79-92ab25399f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You are a TorQ Expert Smart Support bot called 'TESS'.\"),\n",
       " SystemMessage(content='Interpret the following acronyms when answering questions. RDB is an acronym for real time database, WDB is an acronym for write database, CTP is an acronym for chained tickerplant, STP is an acronym for segmented tickerplant, TP is an acronym for tickerplant, and HDB is an acronym for historical database.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75ddad05-11d9-4ba6-9597-7c6fa2a99cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d49f453-cb2b-42a8-9594-95217b9d1c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.table('rag_langchain').drop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
