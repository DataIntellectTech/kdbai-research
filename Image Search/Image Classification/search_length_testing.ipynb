{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44777bd9-cda8-4b90-a813-f4e7bf25f34a",
   "metadata": {},
   "source": [
    "# kdb.ai Classification Network Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f17c79-b2c4-4dad-9827-fa4cd84ddbef",
   "metadata": {},
   "source": [
    "The following document is a template for converting kdb.ai into a classification network based on pre-defined models.\n",
    "\n",
    "This document will be split into 3 parts. The first section will create kdb.ai embeddings on a data set using a model that you have already created. The second step will create embeddings for your test images and is a mandatory step for use of this document. The third step will perfrom classification on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c5ac7d-3043-459c-b43b-2411140be6be",
   "metadata": {},
   "source": [
    "## Section 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d9e5b-2896-468d-83ca-f5089fa163d0",
   "metadata": {},
   "source": [
    "The following section will import all of the required modules and define helper functions that are necessary for this document, and this section should always be run before using the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381434bb-0838-477c-86db-522c70fc120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "926072e8-8be2-45a4-bdb6-8d9e109da092",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ignore tensorflow warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373bc56e-22ea-49ff-a7fe-566c83c9e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# force tensorflow to use CPU only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d4a1b1-66f7-45be-ba01-76a4abc6b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44073db-8ee2-407d-9c2f-e0d2146b0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from huggingface_hub import from_pretrained_keras\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "741ada45-bcb6-4a78-bda2-350c0b31f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea103d05-28fb-4ac8-a733-d657869e1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector DB\n",
    "import kdbai_client as kdbai\n",
    "from getpass import getpass\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b541739-9183-4339-ae4a-ef439768c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import imghdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fdeef97-6d74-4c32-a17d-cc39f36b88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kdbai_client as kdbai\n",
    "session = kdbai.Session(endpoint='http://localhost:8082')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edde2dd7-568f-480d-a17c-5da1348cc138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b71c113-1fd7-43a9-a66d-f5c0db9e10f1",
   "metadata": {},
   "source": [
    "### Defining Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92c0521f-c178-48b5-84c3-c0db05ecbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(df.shape)\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8bb489e-cb42-4917-855e-656c6198404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_paths_from_folder(parent_dir: str) -> dict:\n",
    "    image_paths = {}\n",
    "    for sub_folder in os.listdir(parent_dir):\n",
    "        sub_dir = os.path.join(parent_dir, sub_folder)\n",
    "        image_paths[sub_folder] = [\n",
    "            os.path.join(sub_dir, file) for file in os.listdir(sub_dir)\n",
    "        ]\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2fbb15-d3f9-4e9e-b864-d19788b4436e",
   "metadata": {},
   "source": [
    "## Section 1: Creating Embeddings for Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818148fa-7b31-49f8-b52f-5a624db95429",
   "metadata": {},
   "source": [
    "The following section should be used to create embeddings and store them in kdb.ai. If you have already stored your embeddings in a kdb.ai session, you may skip to section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20ec30-44ce-4ad6-8549-3693b54a2fb3",
   "metadata": {},
   "source": [
    "### IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11154fa4-9e8d-4ecb-b05f-b47043f76b41",
   "metadata": {},
   "source": [
    "The following cell will search your data folder for files that are incompatible with Tensorflow. These files will then be deleted from the data folder, so it is important that if you want to keep all of these images that you have the data set saved elsewhere as a backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d61691f-f218-4403-94d0-cffce790d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/\"\n",
    "image_extensions = [\".png\", \".jpg\", \".jpeg\"]  # add there all your images file extensions\n",
    "\n",
    "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
    "for filepath in Path(data_dir).rglob(\"*\"):\n",
    "    if filepath.suffix.lower() in image_extensions:\n",
    "        img_type = imghdr.what(filepath)\n",
    "        if img_type is None:\n",
    "            print(f\"{filepath} is not an image\")\n",
    "            print(f\"Deleting {filepath} from data folder\")\n",
    "            os.remove(\"{filepath}\")\n",
    "        elif img_type not in img_type_accepted_by_tf:\n",
    "            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")\n",
    "            print(f\"Deleting {filepath} from data folder\")\n",
    "            os.remove(\"{filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c859b-7fd8-4f5f-85d4-b2273eeefcec",
   "metadata": {},
   "source": [
    "### Loading Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c8da3-ad6d-45fa-a45f-37e9da0482b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_map = extract_file_paths_from_folder(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484b2a5-5c01-44d0-8d1d-d8fcbe615756",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = image_dataset_from_directory(\n",
    "    \"data\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    "    seed=1,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837792c6-5642-4914-b9e0-8aa72d06eed9",
   "metadata": {},
   "source": [
    "### Creating Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a1e62-38e6-4d90-9c03-edcd70a2e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model/your_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e460f-34be-43c7-947b-b13901cf95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ce59e-849f-45c5-b824-880f32d092f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty arrays to store the embeddings and labels\n",
    "embeddings = np.empty([len(dataset), 2048])\n",
    "labels = np.empty([len(dataset), 5]) # You must replace N in this line with the number of classifications your data set has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76caaa-e7c2-43ba-9248-8856bfdd0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each image in dataset, get its embedding and class label\n",
    "for i, image in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    embeddings[i, :] = model.predict(image[0], verbose=0)\n",
    "    labels[i, :] = image[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc657179-e992-4d05-9eab-92f359c1c9c8",
   "metadata": {},
   "source": [
    "### Defining Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a671a-67c6-4acd-9074-19f2fd089cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(image_paths_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc88b3e-0a5e-48bc-8438-c8a9078d2b9e",
   "metadata": {},
   "source": [
    "If incorrect classification names are present, use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61267a41-b3dd-4d61-9cc4-498ca9083e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del image_paths_map['.ipynb_checkpoints']\n",
    "sorted(image_paths_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f549133-45f7-4cb6-bd7a-f4310bfae703",
   "metadata": {},
   "source": [
    "And then continue from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84eb33-4e90-41dc-8290-cb3e0d2d4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the classification types in sorted order\n",
    "classification_types = sorted(image_paths_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b18f28-9a8f-4c01-82dc-c234898fd520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each vector, save the classification type given by the high index\n",
    "class_labels = [classification_types[label.argmax()] for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c621d17c-6068-4050-aefd-c09b68f6c894",
   "metadata": {},
   "source": [
    "### Defining Image Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8c061-4dac-4a03-94e8-71cbdb25426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single list of all paths\n",
    "all_paths = []\n",
    "for _, image_paths in image_paths_map.items():\n",
    "    all_paths += image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126989c-d392-43e1-9f7f-c31b896ecc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the source_files in alphanumeric order\n",
    "sorted_all_paths = sorted(all_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba9252-491a-49a3-97c5-d99879f10277",
   "metadata": {},
   "source": [
    "### Defining Embedding Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8f723-ab2f-4eab-9adb-2c2804c405da",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_df = pd.DataFrame(\n",
    "    {\n",
    "        \"source\": sorted_all_paths,\n",
    "        \"class\": class_labels,\n",
    "        \"embedding\": embeddings.tolist(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baafc12-b5c1-435d-a304-cd1713ac24f1",
   "metadata": {},
   "source": [
    "If you receive an error on the previous cell stating that the arrays need to be of the same length, you may need to remove the '.ipynb_checkpoints' from each classification within the data set. The following cells will do this, but it is important that you replicate this cell with as many classifications that you have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c81e2-3cbf-4d64-9724-25305d83453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_all_paths.remove('data/Classname1/.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822d691-804a-4eb8-91a1-56b165ff847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_all_paths.remove('data/Classname2/.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6c78c-1797-4f07-acfb-65407172daad",
   "metadata": {},
   "source": [
    "Do this to remove the files from each classification and then continue from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f73c4e-a2b7-4109-9290-34112c55167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_df(embedded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151a66b-c746-444b-be88-f0b6ef6c55b1",
   "metadata": {},
   "source": [
    "### Defining Vector DB Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9b15b0-8404-438f-95c7-9d102509d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_schema = {\n",
    "    \"columns\": [\n",
    "        {\"name\": \"source\", \"pytype\": \"str\"},\n",
    "        {\"name\": \"class\", \"pytype\": \"str\"},\n",
    "        {\n",
    "            \"name\": \"embedding\",\n",
    "            \"vectorIndex\": {\"dims\": 2048, \"metric\": \"L2\", \"type\": \"hnsw\"},\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40644d4d-2bed-4739-8078-b512280e0084",
   "metadata": {},
   "source": [
    "### Creating Vector DB Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925d18e-0530-4032-b3c2-b67d9ffb25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the table does not already exist\n",
    "try:\n",
    "    session.table(\"yourTable\").drop()\n",
    "    time.sleep(5)\n",
    "except kdbai.KDBAIException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a0028-de88-46d4-974d-7ba5a9ad9505",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = session.create_table(\"efficientNetB7\", image_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a63dd-8216-4ae4-960e-e610460bacc6",
   "metadata": {},
   "source": [
    "### Adding embedded data to the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84258c29-b135-4034-804c-c26838ca69bc",
   "metadata": {},
   "source": [
    "This next stage requires some added steps depending on how large your embedding vector data set is. The \"insert\" command that will be used in this stage can only insert a certain number of bytes, with a general rule of thumb that 10mb is the maximum amount of data that can be inserted at once.\n",
    "\n",
    "The following cell will provide a rough estimate of how many megabytes your embedding vector data set is made up of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f6c0f-d3be-474e-be91-7fe3c4aff716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert bytes to MB\n",
    "embedded_df.memory_usage(deep=True).sum() / (1024**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50fffc-9783-4163-9016-ae4642ac0f0e",
   "metadata": {},
   "source": [
    "If the data set is comfortably below the 10mb limit, then you should be able to insert the embeddings into the table in one step using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f824c-da91-4f1c-875f-a7cab71df757",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.insert(embedded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45e8e4-237a-4a53-bfb5-b7761554fc54",
   "metadata": {},
   "source": [
    "Should the data set be larger than 10mb, you will need to divide the data set into smaller parts. This can be done using the following cells.\n",
    "\n",
    "First of all, it is important to get a rough estimate of how many items will be in each block. This can be done with the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55893bc7-c4b3-4ec8-89fd-a42661ab873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "megab = embedded_df.memory_usage(deep=True).sum() / (1024**2)\n",
    "megabs = megab/10\n",
    "blocks = len(embedded_df)/megabs\n",
    "math.floor(blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f990a4b-7c8a-4d28-ac8f-2644e09220e4",
   "metadata": {},
   "source": [
    "The previous cell will have provided a rough estimate for an upper limit to the amount of items within each block. The following cell will break the data set into blocks of a specified size. Try this with the estimated block size provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47aa63-c63d-4e5e-9bb1-475efa12b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yield successive n-sized \n",
    "# chunks from l. \n",
    "def divide_chunks(l, n): \n",
    "      \n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] \n",
    "  \n",
    "# How many elements each \n",
    "# list should have \n",
    "n = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a0874-f856-4fda-9c55-0e5bcd25a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_df_split = list(divide_chunks(embedded_df, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0c334-fd62-46bb-9eaf-34eb68fa6883",
   "metadata": {},
   "source": [
    "Now that the data set has been split into smaller blocks, it can be inserted into the KDB.AI table using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc53853-2e36-4231-ba32-352ead2cf625",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(embedded_df_split)):\n",
    "    table.insert(embedded_df_split[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f139bf-d353-4992-bffd-bf1a831161c0",
   "metadata": {},
   "source": [
    "Should you still be returned with an error, try breaking the table into smaller blocks than you are currently using and eventually the blocks will be small enough to be inserted into the table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9db4b2-8ec9-4e59-84a9-4dd0155bdbee",
   "metadata": {},
   "source": [
    "You can now verify that the data has been inserted into the table with the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68428bc4-a8bb-47d3-ac18-6cc7f6f0b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328235f-d6fb-4406-a27b-ac029b5f715b",
   "metadata": {},
   "source": [
    "## Section 2: Creating Embeddings for Test Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee5ade-e382-4559-838e-3282ce7ed0b0",
   "metadata": {},
   "source": [
    "Next up, embeddings have to be created for the image that you want to have classified. This will be done in a similar manner to the previous embeddings, but will be classified on the \"search\" folder rather than the data folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d7c1d-bcce-4be1-badf-4ac192c8edac",
   "metadata": {},
   "source": [
    "Should you have already created and inserted data into a table in kdb.ai, you can recall it using the following cell. This is useful as you do not need to recreate the embeddings again each time the model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "090a1c9c-54ec-4bd1-83b4-5b11f6b08539",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = session.table(\"regnetx064\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c019b-9b74-4b94-899d-dc9da9a5547e",
   "metadata": {},
   "source": [
    "Loading in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bd8f2f2-3a21-4acb-bdc7-a6f8959db4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model/your_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebcdd7-8f17-4239-a7b8-3f7cf430a830",
   "metadata": {},
   "source": [
    "### IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3eb25-0755-43ca-8c03-25b63bc7b6fb",
   "metadata": {},
   "source": [
    "Data set testing and deletion occurs with the following cell, please backup the images you do not want to lose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f8bc716-138e-4c02-8093-b7243d5f16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"search/\"\n",
    "image_extensions = [\".png\", \".jpg\", \".jpeg\"]  # add there all your images file extensions\n",
    "\n",
    "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
    "for filepath in Path(data_dir).rglob(\"*\"):\n",
    "    if filepath.suffix.lower() in image_extensions:\n",
    "        img_type = imghdr.what(filepath)\n",
    "        if img_type is None:\n",
    "            print(f\"{filepath} is not an image\")\n",
    "            print(f\"Deleting {filepath} from data folder\")\n",
    "            os.remove(\"{filepath}\")\n",
    "        elif img_type not in img_type_accepted_by_tf:\n",
    "            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")\n",
    "            print(f\"Deleting {filepath} from data folder\")\n",
    "            os.remove(\"{filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53b94c-4c3d-4ef1-9924-996be28b9731",
   "metadata": {},
   "source": [
    "### Loading Search Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a532bb1-65ff-40ae-9d2d-8db53b572ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_image_paths_map = extract_file_paths_from_folder(\"search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b893c44e-bad4-4662-bac5-556cc15acdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "search_dataset = image_dataset_from_directory(\n",
    "    \"search\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    "    seed=1,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6effb-1ab8-4cb7-a1e0-d6a6f2d8f6a7",
   "metadata": {},
   "source": [
    "### Create Search Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1421a82-3668-4b73-bb2b-0624ab121a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_embeddings = np.empty([len(search_dataset), 2048])\n",
    "search_labels = np.empty([len(search_dataset), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d4f5d68-9dc0-4c3c-9c6c-c49cbdb35392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f4e8b57c6b4841bc83059f64af4fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1710251285.035411    5177 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "# for each image in dataset, get its embedding and class label\n",
    "for i, image in tqdm(enumerate(search_dataset), total=len(search_dataset)):\n",
    "    search_embeddings[i, :] = model.predict(image[0], verbose=0)\n",
    "    search_labels[i, :] = image[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a61e0-59c5-4e0b-aeae-c8946209f031",
   "metadata": {},
   "source": [
    "### Defining Test Classification Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48f48df5-a110-46fb-848c-8541eacaecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_classification_types = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21b2e5f5-7910-439b-8aca-c9fbb8c7d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_class_labels = [search_classification_types for search_label in search_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53aef09-a2c4-4c39-b3ac-fcc92f562bb1",
   "metadata": {},
   "source": [
    "### Defining Image Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4cfaa1a-cae6-4127-a527-5dd0a09c8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_paths = []\n",
    "for _, image_paths in search_image_paths_map.items():\n",
    "    search_paths += image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "268a26d1-18df-4bc8-9400-1fc85dca53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_search_paths = sorted(search_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462aeac-93b9-47c9-ae13-0759f119018e",
   "metadata": {},
   "source": [
    "### Defining Embedding Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "595778cf-01b9-4d07-bd3f-1960aae1d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_embedded_df = pd.DataFrame(\n",
    "    {\n",
    "        \"source\": sorted_search_paths,\n",
    "        \"class\": search_class_labels,\n",
    "        \"embedding\": search_embeddings.tolist(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f71cb9-25e4-4bc5-aea8-3067f6b78898",
   "metadata": {},
   "source": [
    "May need to remove the '.ipynb_checkpoints' here too, so this can be done with the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492b631-99db-44f6-b1a0-6f42a5ea2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths.remove('data/test/.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a45711-81ca-4d14-96fc-e095c16bb60d",
   "metadata": {},
   "source": [
    "Then continue from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00bb87da-9070-4429-b4f8-7d9fa9ef0e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>class</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>search/test/healthy (1).jpeg</td>\n",
       "      <td>test</td>\n",
       "      <td>[0.6843213438987732, 0.22339944541454315, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>search/test/healthy (10).jpeg</td>\n",
       "      <td>test</td>\n",
       "      <td>[0.3495194613933563, 0.4964156150817871, 0.059...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>search/test/healthy (100).jpeg</td>\n",
       "      <td>test</td>\n",
       "      <td>[0.3710249960422516, 0.4820334315299988, 0.052...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>search/test/healthy (101).jpeg</td>\n",
       "      <td>test</td>\n",
       "      <td>[0.548697292804718, 0.21445149183273315, 0.096...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>search/test/healthy (102).jpeg</td>\n",
       "      <td>test</td>\n",
       "      <td>[0.6019561886787415, 0.35139545798301697, 0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           source class  \\\n",
       "0    search/test/healthy (1).jpeg  test   \n",
       "1   search/test/healthy (10).jpeg  test   \n",
       "2  search/test/healthy (100).jpeg  test   \n",
       "3  search/test/healthy (101).jpeg  test   \n",
       "4  search/test/healthy (102).jpeg  test   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.6843213438987732, 0.22339944541454315, 0.02...  \n",
       "1  [0.3495194613933563, 0.4964156150817871, 0.059...  \n",
       "2  [0.3710249960422516, 0.4820334315299988, 0.052...  \n",
       "3  [0.548697292804718, 0.21445149183273315, 0.096...  \n",
       "4  [0.6019561886787415, 0.35139545798301697, 0.01...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_df(search_embedded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f9715-7e1c-4e1b-9209-a3630dca7f72",
   "metadata": {},
   "source": [
    "## Section 3: Classifying the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13519ab8-a8e3-49ae-bc33-abe66df3a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding = search_embedded_df.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d643138-9e67-4bb5-bde1-9f3621c3f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = table.search([test_embedding], n=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20163529-1ae7-4725-907d-ff914c676efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = results_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a18de2-173e-4271-882e-d52b91ca231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.mode(results_2.iloc[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e17d1-fa9f-4348-aed5-e1a1229ef067",
   "metadata": {},
   "source": [
    "### Alternate: Classifying multiple images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc2486-aa44-44ae-aeb3-81883b3fc792",
   "metadata": {},
   "source": [
    "The following section can be used to classify a list of images rather than just one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db17bf-0a51-458a-862d-54b1f005968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2d3943-ff8c-4335-ba8e-4ee9ee521999",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications=[]\n",
    "for i in range(len(search_embedded_df)):\n",
    "    w = search_embedded_df.iloc[i,2]\n",
    "    x = table.search([w], n=400)\n",
    "    y = x[0]\n",
    "    z = statistics.mode(y.iloc[:,1])\n",
    "    classifications.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf8af1-a3f2-4b6e-a423-f6316b30f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_list = pd.DataFrame(\n",
    "    {\n",
    "        \"source\": sorted_search_paths,\n",
    "        \"classification\": classifications,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92f9e4-7f54-476d-92ca-3f454bf452ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ecdcc7-aa10-47df-9bb6-3f9356f13656",
   "metadata": {},
   "source": [
    "### Testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46954bd5-b5df-491c-89fe-35d5cf1f1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_classifications=[]\n",
    "for i in range(len(search_embedded_df)):\n",
    "    w = search_embedded_df.iloc[i,0]\n",
    "    x = os.path.basename(w)\n",
    "    y = x.split()\n",
    "    z = y[0]\n",
    "    real_classifications.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c12b6-b612-4de3-8ad2-e543c4e342c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "match=[]\n",
    "for i in range(len(search_embedded_df)):\n",
    "    if real_classifications[i] == (classifications[i]).lower():\n",
    "        match.append('yes')\n",
    "    else:\n",
    "        match.append('no')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70481e0-7046-4898-af8a-e98406ec2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(search_embedded_df)):\n",
    "    if match[i] == 'yes':\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d3257-1db9-4312-b1c7-c0635b2c4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_percentage = (count/(len(search_embedded_df)))*100\n",
    "accuracy_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b8c4d-c344-49a8-b4ec-5d5f9cbdf97e",
   "metadata": {},
   "source": [
    "### Testing best test length for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "553edc10-3dc8-4f62-8f2c-b9457e2d4efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n"
     ]
    }
   ],
   "source": [
    "test_length=[]\n",
    "accuracies=[]\n",
    "\n",
    "for j in range(1, 479):\n",
    "    classifications=[];\n",
    "    for i in range(len(search_embedded_df)):\n",
    "        w = search_embedded_df.iloc[i,2]\n",
    "        x = table.search([w], n=j, index_options={'efSearch': j})\n",
    "        y = x[0]\n",
    "        z = statistics.mode(y.iloc[:,1])\n",
    "        classifications.append(z)\n",
    "    classification_list = pd.DataFrame(\n",
    "    {\n",
    "        \"source\": sorted_search_paths,\n",
    "        \"classification\": classifications,\n",
    "    }\n",
    "    )\n",
    "    real_classifications=[]\n",
    "    for i in range(len(search_embedded_df)):\n",
    "        w = search_embedded_df.iloc[i,0]\n",
    "        x = os.path.basename(w)\n",
    "        y = x.split()\n",
    "        z = y[0]\n",
    "        real_classifications.append(z)\n",
    "    match=[]\n",
    "    for i in range(len(search_embedded_df)):\n",
    "        if real_classifications[i] == (classifications[i]).lower():\n",
    "            match.append('yes')\n",
    "        else:\n",
    "            match.append('no')\n",
    "    count = 0\n",
    "    for i in range(len(search_embedded_df)):\n",
    "        if match[i] == 'yes':\n",
    "            count += 1\n",
    "    accuracy_percentage = (count/(len(search_embedded_df)))*100\n",
    "    test_length.append(j)\n",
    "    accuracies.append(accuracy_percentage) \n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "992a21b3-8aa6-4d3f-aa9b-2d99081e8cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies = pd.DataFrame(\n",
    "    {\n",
    "        \"no. of results\": test_length,\n",
    "        \"accuracy\": accuracies,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d44a380f-8571-4928-939b-a4d2fbbf6d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no. of results</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>81.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no. of results  accuracy\n",
       "6               7      81.6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_row = test_accuracies[test_accuracies['accuracy'] == test_accuracies['accuracy'].max()]\n",
    "max_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4a2cd-2f36-422b-a152-b60dad8200ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
